{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "93d8caea-c5f9-4321-9b56-f7fcc5475675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from models.experimental import attempt_load  # YOLOv7-specific load function\n",
    "from utils.general import non_max_suppression\n",
    "from utils.torch_utils import select_device\n",
    "from PIL import Image\n",
    "\n",
    "# Configure pytesseract path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Add the YOLOv7 repository path to sys.path if necessary\n",
    "sys.path.append('D:/ml_project/yolov7')  # Update this path if required\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a365e65-ecf2-4299-ae63-6df938cdaac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the YOLOv7 model\n",
    "def load_yolo_model(model_path):\n",
    "    device = select_device('')  # Select GPU if available, otherwise CPU\n",
    "    model = attempt_load(model_path, map_location=device)  # Load the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "214bb1c7-75e8-4a52-b6d3-545b59ba2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_ocr(image_path, model, device):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image from {image_path}\")\n",
    "        return\n",
    "\n",
    "    # Get original dimensions of the image\n",
    "    orig_h, orig_w, _ = image.shape\n",
    "\n",
    "    # Convert image to RGB for YOLO compatibility\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Resize image for YOLO input size\n",
    "    input_size = 640\n",
    "    img_resized = cv2.resize(rgb_image, (input_size, input_size))\n",
    "\n",
    "    # Prepare image tensor for YOLO model\n",
    "    img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float().unsqueeze(0) / 255.0\n",
    "    img_tensor = img_tensor.to(device)\n",
    "\n",
    "    # Run YOLO inference\n",
    "    with torch.no_grad():\n",
    "        predictions = model(img_tensor)[0]\n",
    "\n",
    "    # Apply Non-Maximum Suppression\n",
    "    detections = non_max_suppression(predictions, conf_thres=0.25, iou_thres=0.45)\n",
    "\n",
    "    # Scaling factors to map bounding boxes back to the original image size\n",
    "    scale_x = orig_w / input_size\n",
    "    scale_y = orig_h / input_size\n",
    "\n",
    "    extracted_texts = []\n",
    "\n",
    "    for det in detections:\n",
    "        if det is not None and len(det):\n",
    "            for *box, conf, cls in det:  # Each detection includes bbox, confidence, and class\n",
    "                # Scale bounding box coordinates back to original image size\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                x1 = int(x1 * scale_x)\n",
    "                y1 = int(y1 * scale_y)\n",
    "                x2 = int(x2 * scale_x)\n",
    "                y2 = int(y2 * scale_y)\n",
    "\n",
    "                # Add padding to the bounding box\n",
    "                padding_w = int((x2 - x1) * 0.05)  # 15% width padding\n",
    "                padding_h = int((y2 - y1) * 0.05)   # 20% height padding\n",
    "                x1 = max(0, x1 - padding_w)\n",
    "                y1 = max(0, y1 - padding_h)\n",
    "                x2 = min(orig_w, x2 + padding_w)\n",
    "                y2 = min(orig_h, y2 + padding_h)\n",
    "\n",
    "                # Crop the detected number plate\n",
    "                cropped_plate = image[y1:y2, x1:x2]\n",
    "                if cropped_plate.size == 0:\n",
    "                    print(\"Warning: Empty cropped region.\")\n",
    "                    continue\n",
    "\n",
    "                # Debugging: Draw the bounding box on the original image\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.imshow(\"Detected Number Plate\", cropped_plate)\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "                # Preprocess cropped image for OCR\n",
    "                gray_plate = cv2.cvtColor(cropped_plate, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Remove noise using morphological operations\n",
    "                kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "                cleaned_plate = cv2.morphologyEx(gray_plate, cv2.MORPH_CLOSE, kernel)\n",
    "                blurred_plate = cv2.GaussianBlur(cleaned_plate, (5, 5), 0)\n",
    "                binary_plate = cv2.threshold(blurred_plate, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "                # Perform OCR with specific configuration\n",
    "                ocr_config = '--psm 7 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
    "                extracted_text = pytesseract.image_to_string(binary_plate, config=ocr_config)\n",
    "\n",
    "                # Clean up extracted text\n",
    "                clean_text = ''.join(filter(str.isalnum, extracted_text)).upper()\n",
    "                extracted_texts.append(clean_text)\n",
    "\n",
    "    # Show the annotated image with bounding boxes\n",
    "    cv2.imshow(\"Final Annotated Image\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Output extracted texts\n",
    "    for i, text in enumerate(extracted_texts):\n",
    "        print(f\"Extracted Text from Number Plate {i+1}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a02076c8-ff78-4e18-861a-5afeae9be433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "Extracted Text from Number Plate 1: UP14BN4001\n",
      "Extracted Text from Number Plate 2: AMY\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define paths and execute\n",
    "model_path = \"D:/ml_project/yolov7/runs/train/exp/weights/best.pt\"  # Path to YOLOv7 model\n",
    "image_path = \"C:/Users/patel/Downloads/india-rishikesh-automobile-license-plate-D2W2MA.jpg\"  # Path to the test image\n",
    "\n",
    "# Load the YOLOv7 model\n",
    "yolo_model, device = load_yolo_model(model_path)\n",
    "\n",
    "# Perform detection and OCR\n",
    "detect_and_ocr(image_path, yolo_model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72aa986-3d62-445a-824d-ce18ed2dc336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
