{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f72ff50-3c3a-494c-8028-03306737b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "# Set the path to Tesseract OCR\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Add YOLOv7 directory if it's not installed as a package\n",
    "sys.path.append('D:/ml_project/yolov7')  # Update this path to the YOLOv7 directory\n",
    "from models.experimental import attempt_load  # YOLOv7-specific load function\n",
    "from utils.general import non_max_suppression\n",
    "from utils.torch_utils import select_device\n",
    "\n",
    "\n",
    "def preprocess_plate(cropped_plate):\n",
    "    \"\"\"Preprocesses a cropped number plate image to improve OCR accuracy.\"\"\"\n",
    "    gray_plate = cv2.cvtColor(cropped_plate, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Sharpen the image to enhance edges for OCR\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5,-1], [0, -1, 0]])\n",
    "    sharpened_plate = cv2.filter2D(gray_plate, -1, kernel)\n",
    "\n",
    "    # Increase contrast (normalize the image to the range [0,255])\n",
    "    contrast_plate = cv2.convertScaleAbs(sharpened_plate, alpha=2, beta=0)\n",
    "\n",
    "    # Denoise the image for better results\n",
    "    denoised_plate = cv2.fastNlMeansDenoising(contrast_plate, None, 30, 7, 21)\n",
    "\n",
    "    # Threshold to get a binary image\n",
    "    _, binary_plate = cv2.threshold(denoised_plate, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    return binary_plate\n",
    "\n",
    "\n",
    "# Load YOLOv7 model\n",
    "def load_yolo_model(model_path):\n",
    "    device = select_device('cuda' if torch.cuda.is_available() else 'cpu')  # Automatically select device\n",
    "    model = attempt_load(model_path, map_location=device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    return model, device\n",
    "\n",
    "\n",
    "def detect_and_ocr(image_path, model, device, conf_thres=0.25, iou_thres=0.45):\n",
    "    \"\"\"Detect number plates using YOLO and apply OCR.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image from {image_path}\")\n",
    "        return []\n",
    "\n",
    "    orig_h, orig_w, _ = image.shape\n",
    "\n",
    "    # Convert to RGB for YOLO\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    input_size = 640\n",
    "    img_resized = cv2.resize(rgb_image, (input_size, input_size))\n",
    "\n",
    "    # Prepare image for YOLO model\n",
    "    img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float().unsqueeze(0) / 255.0\n",
    "    img_tensor = img_tensor.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(img_tensor)[0]\n",
    "\n",
    "    # Apply Non-Maximum Suppression (NMS)\n",
    "    detections = non_max_suppression(predictions, conf_thres=conf_thres, iou_thres=iou_thres)\n",
    "\n",
    "    scale_x = orig_w / input_size\n",
    "    scale_y = orig_h / input_size\n",
    "\n",
    "    extracted_texts = []\n",
    "\n",
    "    for det in detections:\n",
    "        if det is not None and len(det):\n",
    "            for *box, conf, cls in det:\n",
    "                # Scale bounding box coordinates back to original image size\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                x1 = int(x1 * scale_x)\n",
    "                y1 = int(y1 * scale_y)\n",
    "                x2 = int(x2 * scale_x)\n",
    "                y2 = int(y2 * scale_y)\n",
    "\n",
    "                # Add padding to the bounding box for more robust cropping\n",
    "                padding_w = int((x2 - x1) * 0.15)  # 15% width padding\n",
    "                padding_h = int((y2 - y1) * 0.2)   # 20% height padding\n",
    "                x1 = max(0, x1 - padding_w)\n",
    "                y1 = max(0, y1 - padding_h)\n",
    "                x2 = min(orig_w, x2 + padding_w)\n",
    "                y2 = min(orig_h, y2 + padding_h)\n",
    "\n",
    "                # Crop the detected number plate\n",
    "                cropped_plate = image[y1:y2, x1:x2]\n",
    "                if cropped_plate.size == 0:\n",
    "                    print(\"Warning: Empty cropped region.\")\n",
    "                    continue\n",
    "\n",
    "                # Preprocess the cropped plate for OCR\n",
    "                binary_plate = preprocess_plate(cropped_plate)\n",
    "\n",
    "                # Perform OCR with specific configuration\n",
    "                ocr_config = '--psm 7 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
    "                extracted_text = pytesseract.image_to_string(binary_plate, config=ocr_config)\n",
    "\n",
    "                # Clean up extracted text (remove any non-alphanumeric characters)\n",
    "                clean_text = ''.join(filter(str.isalnum, extracted_text)).upper()\n",
    "                extracted_texts.append(clean_text)\n",
    "\n",
    "                # Debugging: Draw the bounding box on the original image for visual confirmation\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.imshow(\"Cropped Plate\", cropped_plate)\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "    # Show the annotated image with bounding boxes\n",
    "    cv2.imshow(\"Final Annotated Image\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Output extracted texts\n",
    "    for i, text in enumerate(extracted_texts):\n",
    "        print(f\"Extracted Text from Number Plate {i+1}: {text}\")\n",
    "    \n",
    "    return extracted_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29c505ff-1d71-41c1-99c4-c25aed54a740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "Extracted Text from Number Plate 1: 1SH68YME\n",
      "Extracted Texts: ['1SH68YME']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"D:/ml_project/yolov7/runs/train/exp/weights/best.pt\"  # Path to your trained YOLOv7 model\n",
    "    image_path = \"C:/Users/patel/Downloads/car1.jpg\"  # Path to the test image\n",
    "    \n",
    "    # Load YOLO model\n",
    "    yolo_model, device = load_yolo_model(model_path)\n",
    "\n",
    "    # Run detection and OCR\n",
    "    detected_texts = detect_and_ocr(image_path, yolo_model, device)\n",
    "    print(\"Extracted Texts:\", detected_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d5046e-cbcd-4117-b789-cee286ee12e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ml_project\\yolov7\\models\\experimental.py:253: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(w, map_location=map_location)  # load\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from yolov7.models.experimental import attempt_load\n",
    "from yolov7.utils.general import non_max_suppression, scale_coords\n",
    "from yolov7.utils.plots import plot_one_box\n",
    "\n",
    "# Load the model\n",
    "model_path = \"D:/ml_project/yolov7/runs/train/exp/weights/best.pt\"  # Correct model path\n",
    "model = attempt_load(model_path, map_location='cuda')  # Adjust if using GPU\n",
    "model.eval()\n",
    "\n",
    "# Set the confidence threshold\n",
    "confidence_threshold = 0.25  # Adjust based on your requirements\n",
    "iou_threshold = 0.45\n",
    "\n",
    "def run_inference(image_path):\n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Preprocess image to the model's expected input size\n",
    "    img_resized = cv2.resize(img_rgb, (640, 640))\n",
    "    img_tensor = torch.from_numpy(img_resized).float().permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        detections = model(img_tensor)[0]\n",
    "        detections = non_max_suppression(detections, confidence_threshold, iou_threshold)[0]\n",
    "\n",
    "    # Process detections\n",
    "    if detections is not None:\n",
    "        detections[:, :4] = scale_coords(img_tensor.shape[2:], detections[:, :4], img.shape).round()\n",
    "        for *xyxy, conf, cls in detections:\n",
    "            label = f'{int(cls)} {conf:.2f}'\n",
    "            plot_one_box(xyxy, img, label=label, color=[0, 255, 0], line_thickness=2)\n",
    "\n",
    "    # Display the image with detections\n",
    "    cv2.imshow('Detected Objects', img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7041188-0604-4fc9-b483-387dae2725a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "# Set the path for Tesseract executable if needed\n",
    "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def apply_ocr_on_image(image_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Optional: Apply preprocessing techniques to improve OCR accuracy\n",
    "    # gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # Use pytesseract to extract text\n",
    "    extracted_text = pytesseract.image_to_string(gray, config='--psm 8')\n",
    "    \n",
    "    print(\"Extracted Text:\", extracted_text)\n",
    "    return extracted_text\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have a list of cropped number plate images\n",
    "cropped_number_plates = [\"number_plate_1.jpg\", \"number_plate_2.jpg\"]\n",
    "\n",
    "for plate_image in cropped_number_plates:\n",
    "    apply_ocr_on_image(plate_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e98a75f0-6987-491a-8c85-41155ca9dac5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/patel/Downloads/india-rishikesh-automobile-license-plate-D2W2MA.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m run_inference(image_path)\n",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m, in \u001b[0;36mrun_inference\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 27\u001b[0m     detections \u001b[38;5;241m=\u001b[39m model(img_tensor)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     28\u001b[0m     detections \u001b[38;5;241m=\u001b[39m non_max_suppression(detections, confidence_threshold, iou_threshold)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Process detections\u001b[39;00m\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\ml_project/yolov7\\models\\yolo.py:599\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x, augment, profile)\u001b[0m\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# augmented inference, train\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_once(x, profile)\n",
      "File \u001b[1;32mD:\\ml_project/yolov7\\models\\yolo.py:625\u001b[0m, in \u001b[0;36mModel.forward_once\u001b[1;34m(self, x, profile)\u001b[0m\n\u001b[0;32m    622\u001b[0m         dt\u001b[38;5;241m.\u001b[39mappend((time_synchronized() \u001b[38;5;241m-\u001b[39m t) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m    623\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%10.1f\u001b[39;00m\u001b[38;5;132;01m%10.0f\u001b[39;00m\u001b[38;5;132;01m%10.1f\u001b[39;00m\u001b[38;5;124mms \u001b[39m\u001b[38;5;132;01m%-40s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (o, m\u001b[38;5;241m.\u001b[39mnp, dt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], m\u001b[38;5;241m.\u001b[39mtype))\n\u001b[1;32m--> 625\u001b[0m     x \u001b[38;5;241m=\u001b[39m m(x)  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\ml_project/yolov7\\models\\common.py:111\u001b[0m, in \u001b[0;36mConv.fuseforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfuseforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x))\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    551\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "image_path = \"C:/Users/patel/Downloads/india-rishikesh-automobile-license-plate-D2W2MA.jpg\"\n",
    "run_inference(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7be16ea5-9b34-499d-b337-3ae54cc65cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda-installation\\Lib\\site-packages\\torch\\hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Load the YOLO model and run detection\u001b[39;00m\n\u001b[0;32m     48\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/ml_project/yolov7/runs/train/exp/weights/best.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Correct model path\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m yolo_model \u001b[38;5;241m=\u001b[39m load_yolo_model(model_path)\n\u001b[0;32m     50\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/patel/Downloads/car.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m detect_and_ocr(image_path, yolo_model)\n",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m, in \u001b[0;36mload_yolo_model\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_yolo_model\u001b[39m(model_path):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Load the model with custom weights\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124multralytics/yolov7\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m'\u001b[39m, path\u001b[38;5;241m=\u001b[39mmodel_path, force_reload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\site-packages\\torch\\hub.py:638\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown source: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Allowed values: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    635\u001b[0m     )\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 638\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(\n\u001b[0;32m    639\u001b[0m         repo_or_dir,\n\u001b[0;32m    640\u001b[0m         force_reload,\n\u001b[0;32m    641\u001b[0m         trust_repo,\n\u001b[0;32m    642\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    643\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    644\u001b[0m         skip_validation\u001b[38;5;241m=\u001b[39mskip_validation,\n\u001b[0;32m    645\u001b[0m     )\n\u001b[0;32m    647\u001b[0m model \u001b[38;5;241m=\u001b[39m _load_local(repo_or_dir, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\site-packages\\torch\\hub.py:258\u001b[0m, in \u001b[0;36m_get_cache_or_reload\u001b[1;34m(github, force_reload, trust_repo, calling_fn, verbose, skip_validation)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;66;03m# Validate the tag/branch is from the original repo instead of a forked repo\u001b[39;00m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_validation:\n\u001b[1;32m--> 258\u001b[0m         _validate_not_a_forked_repo(repo_owner, repo_name, ref)\n\u001b[0;32m    260\u001b[0m     cached_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hub_dir, normalized_br \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    261\u001b[0m     _remove_if_exists(cached_file)\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\site-packages\\torch\\hub.py:203\u001b[0m, in \u001b[0;36m_validate_not_a_forked_repo\u001b[1;34m(repo_owner, repo_name, ref)\u001b[0m\n\u001b[0;32m    201\u001b[0m page \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    202\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?per_page=100&page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 203\u001b[0m response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(_read_url(Request(url, headers\u001b[38;5;241m=\u001b[39mheaders)))\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Empty response means no more data to process\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response:\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\site-packages\\torch\\hub.py:185\u001b[0m, in \u001b[0;36m_read_url\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_url\u001b[39m(url):\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m urlopen(url) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget_content_charset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\urllib\\request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\urllib\\request.py:521\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    520\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 521\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\urllib\\request.py:630\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 630\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\urllib\\request.py:559\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    558\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\urllib\\request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mD:\\conda-installation\\Lib\\urllib\\request.py:639\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import torch\n",
    "\n",
    "# Load YOLO model using PyTorch's torch.hub for YOLOv7\n",
    "def load_yolo_model(model_path):\n",
    "    # Load the model with custom weights\n",
    "    model = torch.hub.load('ultralytics/yolov7', 'custom', path=model_path, force_reload=True)\n",
    "    return model\n",
    "\n",
    "# Function to apply YOLO, crop number plates, and then apply OCR\n",
    "def detect_and_ocr(image_path, yolo_model):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert image to RGB as YOLO expects RGB format\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Run YOLO detection on the image\n",
    "    results = yolo_model(rgb_image)\n",
    "    \n",
    "    # Process each detection\n",
    "    for *box, conf, class_id in results.xyxy[0]:  # xyxy format bounding box\n",
    "        # Assuming the model is trained only for number plates or using the class ID\n",
    "        if int(class_id) == 0:  # Change as needed if number plates are a different class ID\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            \n",
    "            # Crop the detected number plate from the image\n",
    "            number_plate = image[y1:y2, x1:x2]\n",
    "            \n",
    "            # Convert the cropped image to grayscale for better OCR results\n",
    "            gray_plate = cv2.cvtColor(number_plate, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Optional: Apply thresholding to improve OCR accuracy\n",
    "            gray_plate = cv2.threshold(gray_plate, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "            \n",
    "            # Use pytesseract to extract text from the cropped image\n",
    "            extracted_text = pytesseract.image_to_string(gray_plate, config='--psm 8')\n",
    "            \n",
    "            print(\"Extracted Text from Number Plate:\", extracted_text)\n",
    "            # Display cropped number plate for verification (optional)\n",
    "            cv2.imshow(\"Cropped Number Plate\", number_plate)\n",
    "            cv2.waitKey(0)\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Load the YOLO model and run detection\n",
    "model_path = '/home/shivam/Desktop/ml_project/yolov7/runs/train/exp/weights/best.pt'\n",
    "yolo_model = load_yolo_model(model_path)\n",
    "image_path = '/home/shivam/Downloads/ferrari-188954_1280.jpg'\n",
    "detect_and_ocr(image_path, yolo_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb03a84b-eabd-438b-9e97-e6586270b4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
